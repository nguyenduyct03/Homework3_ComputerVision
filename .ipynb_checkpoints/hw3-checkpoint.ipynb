{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "This assignment covers Harris corner detector, RANSAC and HOG descriptor.\n",
    "\n",
    "Bài tập này bao gồm: bộ phát hiện góc Harris, phương pháp RANSAC và bộ mô tả đặc trưng HOG. Dùng nó để làm một ứng dụng ghép hình Pananoma đơn giản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (cell_name, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"cell_name\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print (\"NguyenAnhDuyB1509854\")\n",
    "print (\"Họ va ten 2 (MSSV)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Panorama Stitching\n",
    "Panorama stitching is an early success of computer vision. Matthew Brown and David G. Lowe published a famous [panoramic image stitching paper](http://matthewalunbrown.com/papers/ijcv2007.pdf) in 2007. Since then, automatic panorama stitching technology has been widely adopted in many applications such as Google Street View, panorama photos on smartphones,\n",
    "and stitching software such as Photosynth and AutoStitch.\n",
    "\n",
    "Ghép ảnh panorama là một trong ứng dụng thành công đầu tiên của thị giác máy tính. Mathew Brown và David G. Lowe đã công bố một bài báo nổi tiếng vào năm 2007. Từ đó, công nghệ ghép ảnh panorama tự động đã được sử dụng rộng rãi trong nhiều ứng dụng như Google Street View, ảnh panorama trên điện thoại thông minh và các phần mềm ghép ảnh như Photosynth and AutoStitch.\n",
    "\n",
    "In this assignment, we will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:\n",
    "\n",
    "Trong bài tập này, ta sẽ phát hiện so khớp (đối sánh) các keyoints từ nhiều ảnh và ghép thành một ảnh panorama duy nhất. Quá trình này gồm nhiều bước:\n",
    "\n",
    "1. Use Harris corner detector to find keypoints.\n",
    "\n",
    "Sử dụng bộ phát hiện góc Harris để tìm các keypoints\n",
    "\n",
    "2. Build a descriptor to describe each point in an image. <br>\n",
    "   Compare two sets of descriptors coming from two different images and find matching keypoints.\n",
    "\n",
    "Với mỗi keypoint xây dựng một bộ mô tả cho nó (tính vector đặc trưng cho mỗi keypoint).<br>\n",
    "So sánh hai tập các vector đặc trưng của hai ảnh để tìm các cặp keypoints khớp với nhau.\n",
    "\n",
    "3. Given a list of matching keypoints, use least-squares method to find the affine transformation matrix that maps points in one image to another.\n",
    "\n",
    "Với một danh sách các cặp keypoint khớp với nhau đã cho, sử dụng phương pháp bình phương tối thiểu để tìm biến đổi affine (một phép biến đổi hình học) để biến đổi những điểm trong ảnh này thành những điểm tương ứng trong điểm kia.\n",
    "\n",
    "4. Use RANSAC to give a more robust estimate of affine transformation matrix. <br>\n",
    "   Given the transformation matrix, use it to transform the second image and overlay it on the first image, forming a panorama.\n",
    "   \n",
    "Sử dụng kỹ thuật RANSAC để tìm một ma trận biến đổi affine tốt nhất (ta đã sử dụng kỹ thuật này trong tìm đường thẳng tốt nhất, nay áp dụng nó để tìm ma trận tốt nhất).<br>\n",
    "Với ma trận biến đổi tìm thầy, sử dụng nó để biến đổi ảnh thứ hai và chồng nó lên ảnh thứ nhất. Như thế sẽ tạp nên ảnh panorama !\n",
    "   \n",
    "5. Implement a different descriptor (HOG descriptor) and get another stitching result.\n",
    "\n",
    "Trong phần 2, đầu tiên ta sẽ sử dụng một bộ mô tả rất đơn giản, đó là: trải dài vùng ảnh xung quanh keypoint để thu được một vector các mức xám của các điểm này. Vector đặc trưng thu được từ bộ mô tả này thường không có tính phân biệt cao nên kết quả thường không tốt. Trong phần này, ta cài đặt thêm một bộ mô tả (vector đặc trưng) khác nữa, đó là bộ mô tả HOG. Và làm lại quá trình ghép ảnh, ta sẽ thu được kết quả tốt hơn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Harris Corner Detector (20 points)\n",
    "\n",
    "\n",
    "In this section, you are going to implement Harris corner detector for keypoint localization. Review the lecture slides on Harris corner detector to understand how it works. The Harris detection algorithm can be divide into the following steps:\n",
    "\n",
    "Trong phần này, bạn sẽ cài đặt bộ phát hiện góc Harris để định vị các keypoint. Hãy xem lại các slides trong phần lý thuyết về bộ phát hiện góc Harris (các slides 43 - 62) để hiểu thêm về các thức hoạt động của nó. Giải thuật phát hiện góc Harris gồm các bước sau:\n",
    "\n",
    "1. Compute $x$ and $y$ derivatives ($I_x, I_y$) of an image\n",
    "\n",
    "Tính đạo hàm riêng theo $x$ và $y$ ($I_x, I_y$) của ảnh\n",
    "\n",
    "2. Compute products of derivatives ($I_x^2, I_y^2, I_{xy}$) at each pixel\n",
    "\n",
    "Tính tích các đạo hàm ($I_x^2, I_y^2, I_{xy}$) tại mỗi điểm\n",
    "\n",
    "$$\n",
    "I_x^2 = I_x.I_x\\\\\n",
    "I_y^2 = I_y.I_y\\\\\n",
    "I_{xy} = I_x.I_y\n",
    "$$\n",
    "\n",
    "3. Compute matrix $M$ at each pixel, where\n",
    "\n",
    "Với mỗi điểm ảnh (x,y) tính ma trận $M$ theo công thức\n",
    "\n",
    "$$\n",
    "M = \\sum_{x,y} w(x,y)\n",
    "    \\begin{bmatrix}\n",
    "        I_{x}^2 & I_{x}I_{y} \\\\\n",
    "        I_{x}I_{y} & I_{y}^2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "$M$ là một ma trận $2\\times2$.\n",
    "\n",
    "4. Compute corner response $R=Det(M)-k(Trace(M)^2)$ at each pixel\n",
    "\n",
    "Tính ma trận kết quả R bằng cách tính $R=Det(M)-k(Trace(M)^2)$ tại mỗi điểm.\n",
    "\n",
    "5. Output corner response map $R(x,y)$\n",
    "\n",
    "Trả về ma trận $R(x,y)$.\n",
    "\n",
    "Step 1 is already done for you in the function **`harris_corners`** in `panorama.py`. Complete the function implementation and run the code below.\n",
    "\n",
    "Trong hàm **`harris_corners`** trong `panorama.py`, người ta đã làm xong bước 1 rồi. Hãy hoàn tất các bước còn lại và chạy đoạn code bên dưới.\n",
    "\n",
    "*-Hint: You may use the function `scipy.ndimage.filters.convolve`, which is already imported in `panoramy.py`*\n",
    "\n",
    "*-Gợi ý: Có thể bạn cần dùng tới hàm `scipy.ndimage.filters.convolve`. Hàm này đã được import trong `panoramy.py` rồi.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imread' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f985d3d08715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpanorama\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mharris_corners\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sudoku.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_grey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Compute Harris corner response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imread' is not defined"
     ]
    }
   ],
   "source": [
    "from panorama import harris_corners\n",
    "\n",
    "img = imread('sudoku.png', as_grey=True)\n",
    "\n",
    "# Compute Harris corner response\n",
    "response = harris_corners(img)\n",
    "\n",
    "# Display corner response\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(response)\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Response')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imread('solution_harris.png', as_grey=True))\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Solution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you implement the Harris detector correctly, you will be able to see small bright blobs around the corners of the sudoku grids and letters in the output corner response image. The function `corner_peaks` from `skimage.feature` performs non-maximum suppression to take local maxima of the response map and localize keypoints.\n",
    "\n",
    "Một khi bạn đã cài đúng bộ phát hiện góc Harris, bạn sẽ thấy một những đốm sáng nhỏ xung quanh các góc của lưới sudoku và góc của các ký tự trong ảnh kết quả R. Hàm `corner_peaks` từ `skimage.feature` thực hiện xoá các điểm không cực trị giữ lại các điểm cực trị cục bộ và định vị các keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform non-maximum suppression in response map\n",
    "# and output corner coordiantes\n",
    "corners = corner_peaks(response, threshold_rel=0.01)\n",
    "\n",
    "# Display detected corners\n",
    "plt.imshow(img)\n",
    "plt.scatter(corners[:,1], corners[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Corners')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Describing and Matching Keypoints (20 points)\n",
    "\n",
    "We are now able to localize keypoints in two images by running the Harris corner detector independently on them. Next question is, how do we determine which pair of keypoints come from corresponding locations in those two images? In order to *match* the detected keypoints, we must come up with a way to *describe* the keypoints based on their local appearance. Generally, each region around detected keypoint locations is converted into  a fixed-size vectors called *descriptors*.\n",
    "\n",
    "Sau khi đã cài đặt được giải bộ phát hiện góc Harris, giờ đây ta có thể định vị được các keypoints trên hai ảnh bằng cách gọi hàm **harris_corners** độc lập trên từng ảnh. Câu hỏi tiếp theo là làm thể nào để xác định đượccasc cặp keypoint tương ứng với nhau trong hai ảnh này? Để *so khớp* các keypoint, ta phải tìm cách *mô tả* các keypoint dựa trên vùng ảnh xung quanh keypoint. Một cách tổng quát, vùng ảnh xung quanh keypoint sẽ được biến đổi thành một vector có kích thước giống nhau, và gọi là *bộ mô tả* hay *vector đặc trưng* của keypoint.\n",
    "\n",
    "\n",
    "### Part 2.1 Creating Descriptors (10 points)\n",
    "\n",
    "In this section, you are going to implement a **`simple_descriptor`**; each keypoint is described by normalized intensity in a small patch around it.\n",
    "\n",
    "Trong phần này, bạn sẽ cài đặt một **`bộ mô tả đơn giản`**; mỗi keypoint được mô tả bằng cường độ sáng (mức xám) chuẩn hoá của các pixel trong một vùng ảnh nhỏ xung quanh keypoint. Các bước thực hiện:\n",
    "\n",
    "- Trải dài vùng ảnh patch kích thước (H x W) thành một mảng 1 chiều kích thước H\\*W, gọi mảng này là **`feature`**.\n",
    "- Tính giá trị trung bình của các phần tử của **`feature`**\n",
    "- Tính độ lệch chuẩn của các phần tử của **`feature`**\n",
    "- Chuẩn hoá các phần tử của **`feature`** bằng cách trừ cho giá trị trung bình và lấy kết quả chia cho độ lệch chuẩn.\n",
    "\n",
    "Hãy viết hàm **`simple_descriptor`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from panorama import harris_corners\n",
    "\n",
    "img1 = imread('uttower1.jpg', as_grey=True)\n",
    "img2 = imread('uttower2.jpg', as_grey=True)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(keypoints1[:,1], keypoints1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(keypoints2[:,1], keypoints2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 Matching Descriptors (10 points)\n",
    "Then, implement **`match_descriptors`** function to find good matches in two sets of descriptors. First, calculate Euclidean distance between all pairs of descriptors from image 1 and image 2. Then use this to determine if there is a good match: if the distance to the closest vector is significantly (by a factor which is given) smaller than the distance to the second-closest, we call it a match. The output of the function is an array where each row holds the indices of one pair of matching descriptors.\n",
    "\n",
    "\n",
    "Tiếp theo, cài hàm **`match_descriptors`** để tìm các cặp keypoint khớp nhau từ hai tập hợp các bộ mô tả. Trước hết, tính khoảng cách Euclide giữa tất cả các cặp mô tả từ ảnh 1 và ảnh 2. Sau đó, dùng các khoảng cách này để tìm các cặp keypoint khớp nhau.\n",
    "\n",
    "Giả sử ta đang xét keypoint i của ảnh 1 có bộ mô tả là desc1[i,].\n",
    "- Ta tìm hai keypoint trong ảnh 2 có bộ mô tả gần với desc1[i, ] nhất. Gọi keypoint gần nhất là j và keypoint gần nhì là k (hay dist[i, j] < dist[i, k]).\n",
    "- Nếu dist[i, j] **nhỏ hơn nhiều** so với dist[i, k] thì (i, j) là một cặp keypoint khớp với nhau. Trong bản cài đặt này, khái niệm **nhỏ hơn nhiều** được hiểu là tỷ lệ của hai khoảng cách nhỏ hơn một ngưỡng nào đó (theo công thức của David G. Lowe):\n",
    "$$dist[i, j]/dist[i, k] < threshold$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from panorama import simple_descriptor, match_descriptors, describe_keypoints\n",
    "from utils import plot_matches\n",
    "\n",
    "patch_size = 5\n",
    "\n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "plt.imshow(imread('solution_simple_descriptor.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Matched Simple Descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Transformation Estimation (20 points)\n",
    "\n",
    "We now have a list of matched keypoints across the two images. We will use this to find a transformation matrix that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that\n",
    "\n",
    "Giờ đây ta đã có một danh sách các cặp keypoint khớp nhau của hai ảnh. Ta sẽ dùng nó để tìm một ma trận biến đổi hình học để biến đổi các điểm của ảnh 2 thành các điểm tương ứng trong hệ toạ độ của ảnh 1. Nói cách khác, nếu điểm $p_1 = [y_1,x_1]$ trong ảnh 1 khới với điểm $p_2=[y_2, x_2]$ trong ảnh 2, ta sẽ tìm một ma trận biến đổi hình học (biến đổi affine) $H$ sao cho\n",
    "\n",
    "$$\n",
    "\\tilde{p_2}H = \\tilde{p_1},\n",
    "$$\n",
    "\n",
    "where $\\tilde{p_1}$ and $\\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.\n",
    "\n",
    "trong đó $\\tilde{p_1}$ và $\\tilde{p_2}$ là toạ độ thuần nhất của $p_1$ $p_2$ (xem lại môn đồ hoạ máy tính để biết thêm về toạ độ thuần nhất).\n",
    "\n",
    "\n",
    "Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, we can estimate the transformation matrix with least squares. Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \\times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,\n",
    "\n",
    "Chú ý là có thể bạn sẽ không tìm được ma trận $H$ để biến đổi chính xác mỗi điểm trong ảnh 2 thành 1 điểm tương ứng trong ảnh 1. Tuy nhiên, ta có thể tìm được một ma trận xấp xỉ gần đúng với ma trận lý tưởng bằng phương pháp bình phương nhỏ nhất. Cho $N$ cặp keypoint khớp nhau, gọi $X_1$ và $X_2$ là các ma trận kích thước $N \\times 3$ với hàng của nó là các toạ độ thuần nhất của các keypoints tương ứng trong ảnh 1 và ảnh 2. Ta có thể ước lượng $H$ bằng cách giải bài toán bình phương nhỏ nhất sau:\n",
    "\n",
    "$$\n",
    "X_2 H = X_1\n",
    "$$\n",
    "\n",
    "Implement **`fit_affine_matrix`** in `panorama.py`\n",
    "\n",
    "*-Hint: read the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) about np.linalg.lstsq*\n",
    "\n",
    "\n",
    "*-Gợi ý:\n",
    "Đây là một bài toán giải hệ phương trình tuyến tính có số phương trình nhiều hơn biến. Bạn có thể sử dụng phương pháp nghịch đảo ma trận giả (pseudo-inverse) hoặc phép chia ma trận để tìm $H$. Xem thêm tài liệu về bước này trong bộ slides **03_linalg_review**.\n",
    "Nên đọc thêm tài liệu [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) về hàm giải bài toán bình phương nhỏ nhất **np.linalg.lstsq***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from panorama import fit_affine_matrix\n",
    "\n",
    "# Sanity check for fit_affine_matrix\n",
    "\n",
    "# Test inputs\n",
    "a = np.array([[0.5, 0.1], [0.4, 0.2], [0.8, 0.2]])\n",
    "b = np.array([[0.3, -0.2], [-0.4, -0.9], [0.1, 0.1]])\n",
    "\n",
    "H = fit_affine_matrix(b, a)\n",
    "\n",
    "# Target output\n",
    "sol = np.array(\n",
    "    [[1.25, 2.5, 0.0],\n",
    "     [-5.75, -4.5, 0.0],\n",
    "     [0.25, -1.0, 1.0]]\n",
    ")\n",
    "\n",
    "error = np.sum((H - sol) ** 2)\n",
    "\n",
    "if error < 1e-20:\n",
    "    print('Implementation correct!')\n",
    "else:\n",
    "    print('There is something wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking that your `fit_affine_matrix function` is running correctly, run the following code to apply it to images.\n",
    "Images will be warped and image 2 will be mapped to image 1. Then, the two images are merged to get a panorama. Your panorama may not look good at this point, but we will later use other techniques to get a better result.\n",
    "\n",
    "Sau khi kiểm tra hàm `fit_affine_matrix function` chạy chính xác, hãy chạy đoạn code bên dưới để áp dụng nó vào các ảnh.\n",
    "Các ảnh sẽ được biến đổi và ảnh 2 sẽ được ánh xạ lên ảnh 1. Sau đó, hai ảnh sẽ được trộn (ghép) với nhau để tạo thành ảnh panorama. Tại thời điểm này ảnh panorama kết quả của bạn có thể chưa đẹp lăm, nhưng chút nữa ta sẽ làm cho nó tốt hơn bằng cách dùng các kỹ thuật khác.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import get_output_space, warp_image\n",
    "\n",
    "# Extract matched keypoints\n",
    "p1 = keypoints1[matches[:,0]]\n",
    "p2 = keypoints2[matches[:,1]]\n",
    "\n",
    "# Find affine transformation matrix H that maps p2 to p1\n",
    "H = fit_affine_matrix(p1, p2)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 RANSAC (20 points)\n",
    "Rather than directly feeding all our keypoint matches into ``fit_affine_matrix`` function, we can instead use RANSAC (\"RANdom SAmple Consensus\") to select only \"inliers\" to use to compute the transformation matrix.\n",
    "\n",
    "Thay vì sử dụng toàn bộ các cặp keypoint khớp với nhau đưa vào hàm ``fit_affine_matrix``, ta có thể dùng kỹ thuật RANSAC (\"RANdom SAmple Consensus\") để chỉ chọn những \"inliers\" (tức các cặp keypoint) phù hợp để tìm ma trận biến đổi $H$.\n",
    "\n",
    "The steps of RANSAC are:\n",
    "\n",
    "Các bước của RANSAC bao gồm:\n",
    "\n",
    "    1. Select random set of matches\n",
    "    \n",
    "    Chọn ngẫu nhiên một số cặp điểm (ví dụ: 4 cặp)\n",
    "    \n",
    "    2. Compute affine transformation matrix\n",
    "    \n",
    "    Tính ma trận biến đổi $H$ từ các cặp điểm đã chọn\n",
    "    \n",
    "    3. Find inliers using the given threshold\n",
    "    \n",
    "    Tìm các inliers với một ngưỡng cho trước. Một cặp điểm (x1, x2) được gọi là inliers nếu khoảng cách từ điểm x2H đến  x1 < ngưỡng\n",
    "    \n",
    "    4. Repeat and keep the largest set of inliers\n",
    "    \n",
    "    Lặp lại quá trình này (các bước từ 1 đến 3) sau một số lần và giữ lại tập inliers có nhiều phần tử nhất\n",
    "    \n",
    "    5. Re-compute least-squares estimate on all of the inliers\n",
    "    \n",
    "    Tính lại ma trận $H$ từ tập các inliers nhiều nhất.\n",
    "\n",
    "Implement **`ransac`** in `panorama.py`, run through the following code to get a panorama. You can see the difference from the result we get without RANSAC.\n",
    "\n",
    "Hãy cài đặt hàm **`ransac`** trong `panorama.py`, chạy lại đoạn code bên dưới để có được ảnh ghép panorama. Bạn sẽ thấy sự khác nhau giữa có và không sử dụng RANSAC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from panorama import ransac\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "\n",
    "# Visualize robust matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Histogram of Oriented Gradients (HOG) (20 points)\n",
    "In the above code, you are using the `simple_descriptor`, and in this section, you are going to implement a simplified version of HOG descriptor. <br>\n",
    "HOG stands for Histogram of\tOriented Gradients. In HOG descriptor, the distribution ( histograms ) of directions of gradients ( oriented gradients ) are used as features. Gradients ( x and y derivatives ) of an image are useful because the magnitude of gradients is large around edges and corners ( regions of abrupt intensity changes ) and we know that edges and corners pack in a lot more information about object shape than flat regions.<br>\n",
    "The steps of HOG are:\n",
    "\n",
    "Trong phần code bên trên, bạn đang sử dụng `simple_descriptor`, và trong phần này, bạn sẽ cài đặt một phiên bản đơn giản hoá của bộ mô tả HOG. <br>\n",
    "HOG là từ viết tắt của Histogram of\tOriented Gradients (Lược đồ phân bố gradient có hướng). Trong bộ mô tả HOG, phân phối của gradient (có hướng) được dùng như đặc trưng. Gradients (đạo hàm riêng theo x và y) của ảnh là một đặc trưng quan trọng vì độ lớn của gradient sẽ lơn tại các đường biên và góc (vùng có sự thay đổi đột về cường độ sáng) và ta cũng biết rằng cung và góc chứa rất nhiều thông tin về hình dáng của đối tượng hơn là các vùng không có thay độ sáng.<br>\n",
    "Các bước của giải thuật tính bộ mô tả HOG bao gồm:\n",
    "\n",
    "    1. compute the gradient image in x and y\n",
    "        Use the Sobel filter provided by skimage.filters\n",
    "        \n",
    "    Tính đạo hàm ảnh theo x và y\n",
    "        Sử dụng bộ lọc Sobel trong module skimage.filters\n",
    "        \n",
    "    2. compute gradient histograms\n",
    "        Divide image into cells, and calculate histogram of gradient in each cell.\n",
    "        \n",
    "    Tính lược đồ phân bố gradient\n",
    "        Chia ảnh thành các ô, và tính lược đồ phân bố gradient trong mỗi ô.\n",
    "        \n",
    "    3. normalize across block\n",
    "        Normalize the histogram so that they\n",
    "        \n",
    "    Chuẩn hoá các khối\n",
    "        Chuẩn hoá lược đồ phân bố sao cho chúng đều nhau\n",
    "        \n",
    "    4. flattening block into a feature vector\n",
    "    \n",
    "    Trải rộng các khối thành một vector đặc trưng duy nhất\n",
    "\n",
    "Implement **`hog_descriptor`** in `panorama.py`, and run through the following code to get a panorama image.\n",
    "\n",
    "Hãy cài đặt hàm **`hog_descriptor`** trong `panorama.py`, và chạy đoạn code bên dưới để có ảnh panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from panorama import hog_descriptor\n",
    "\n",
    "img1 = imread('uttower1.jpg', as_grey=True)\n",
    "img2 = imread('uttower2.jpg', as_grey=True)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=hog_descriptor,\n",
    "                           patch_size=16)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=hog_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "plt.imshow(imread('solution_hog.png'))\n",
    "plt.axis('off')\n",
    "plt.title('HOG descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from panorama import ransac\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_hog_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('HOG descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_hog_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('HOG Descriptor Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Better Image Merging\n",
    "You will notice the blurry region and unpleasant lines in the middle of the final panoramic image. In the cell below, come up with a better merging scheme to make the panorama look more natural. Be creative!\n",
    "\n",
    "Nếu chú bý ạn sẽ thấy những vùng bị nhờ và các đường thẳng nằm ở giữa ảnh panorama. Đoạn code bên dưới mô tả một cách để ghép ảnh đẹp hơn, tự nhiên hơn. Bạn hãy tự mình sáng tạo thêm các cách khác nhé !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify the code below\n",
    "\n",
    "### YOUR CODE HERE\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "overlap = (img1_mask * 1.0 + img2_mask)\n",
    "\n",
    "output = merged / np.maximum(overlap, 1)\n",
    "### END YOUR CODE\n",
    "\n",
    "plt.imshow(output)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extra Credit: Stitching Multiple Images\n",
    "Work in the cell below to complete the code to stitch an ordered chain of images.\n",
    "\n",
    "Given a sequence of $m$ images ($I_1, I_2,...,I_m$), take every neighboring pair of images and compute the transformation matrix which converts points from the coordinate frame of $I_{i+1}$ to the frame of $I_{i}$. Then, select a reference image $I_{ref}$, which is in the middle of the chain. We want our final panorama image to be in the coordinate frame of $I_{ref}$. So, for each $I_i$ that is not the reference image, we need a transformation matrix that will convert points in frame $i$ to frame $ref$.\n",
    "\n",
    "\n",
    "Phần sau đây mô tả quy trình ghép nhiều ảnh (thay vì 2).\n",
    "\n",
    "Giả sử đầu vào là $m$ ảnh theo thứ tự ($I_1, I_2,...,I_m$), ta lấy hai ảnh liên tiếp và tính ma trận biến đổi $H$ để chuyển từ hệ toạ độ của ảnh $I_{i+1}$ thành $I_{i}$. Sau đó, chọn một ảnh tham chiếu $I_{ref}$, nên chọn ảnh chính giữa $I_{m/2}$. Ta muốn ảnh panorama kết quả sẽ lấy theo toạ độ của ảnh $I_{ref}$. Vì thế, với mỗi ảnh $I_i$ không phải ảnh tham chiếu, ta cần một ma trận biến đổi để biến đổi các điểm trong ảnh $i$ về ảnh $ref$.\n",
    "\n",
    "\n",
    "*-Hint:*\n",
    "- If you are confused, you may want to review the Linear Algebra slides on how to combine the effects of multiple transformation matrices.\n",
    "- The inverse of transformation matrix has the reverse effect. Please use [`numpy.linalg.inv`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) function whenever you want to compute matrix inverse.\n",
    "\n",
    "*-Gợi ý:*\n",
    "- Nếu bạn còn mơ hồ, hãy xem lại các slides liên quan đến đại số tuyến tính về cách tìm nhiều ma trận biến đổi.\n",
    "- Nghịch đảo của ma trận biến đổi có thể bị hiệu ứng lề. Hãy sử dụng hàm [`numpy.linalg.inv`] thay vì sử dụng phép nghịch đảo (https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) khi muốn tính nghịch đảo ma trận."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img1 = imread('yosemite1.jpg', as_grey=True)\n",
    "img2 = imread('yosemite2.jpg', as_grey=True)\n",
    "img3 = imread('yosemite3.jpg', as_grey=True)\n",
    "img4 = imread('yosemite4.jpg', as_grey=True)\n",
    "\n",
    "# Detect keypoints in each image\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints3 = corner_peaks(harris_corners(img3, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints4 = corner_peaks(harris_corners(img4, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "# Describe keypoints\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc3 = describe_keypoints(img3, keypoints3,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc4 = describe_keypoints(img4, keypoints4,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "# Match keypoints in neighboring images\n",
    "matches12 = match_descriptors(desc1, desc2, 0.7)\n",
    "matches23 = match_descriptors(desc2, desc3, 0.7)\n",
    "matches34 = match_descriptors(desc3, desc4, 0.7)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize final panorama image\n",
    "plt.imshow(panorama)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
